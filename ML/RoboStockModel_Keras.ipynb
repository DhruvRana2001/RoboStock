{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, BackupAndRestore, TensorBoard\n",
    "from keras.layers import Input, LSTM, Dense, Dropout, Permute, Lambda, Multiply, Activation, GRU, Flatten, Conv1D, MaxPooling1D, Flatten, Average, BatchNormalization, Conv2D, MaxPooling2D\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support,mean_squared_error\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using my GPU to accelrate the results\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = np.load('shapes.npy',allow_pickle=True)\n",
    "X_shape = shapes.item()['X']\n",
    "y_shape = shapes.item()['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.memmap('X_sequence.dat', dtype='float64', mode='r+', shape=X_shape)\n",
    "Y = np.memmap('Y_sequence.dat', dtype='float64', mode='r+', shape=y_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, X_path, Y_path, shapes , indices, batch_size=32,shuffle=True):\n",
    "\n",
    "        self.X = np.memmap(X_path, dtype='float64', mode='r+', shape=shapes.item()['X'])\n",
    "        self.Y = np.memmap(Y_path, dtype='float64', mode='r+', shape=shapes.item()['y'])\n",
    "\n",
    "        self.indices = indices\n",
    "        self.num_samples = len(self.indices)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.num_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X = self.X[batch_indices]\n",
    "        Y = self.Y[batch_indices]\n",
    "        return X,Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Model(input_size, output_shape, hidden_size=64, dropout=0.2):\n",
    "    inputs = Input(shape=(input_size[1], input_size[2])) # (batch_size, time_step, features)\n",
    "    \n",
    "    # LSTM layers\n",
    "    lstm_out = LSTM(hidden_size, return_sequences=True)(inputs)\n",
    "    lstm_out = Dropout(dropout)(lstm_out)\n",
    "\n",
    "    lstm_out = LSTM(int(hidden_size*2), return_sequences=True)(lstm_out)\n",
    "    lstm_out = Dropout(dropout+0.1)(lstm_out)\n",
    "\n",
    "    lstm_out = LSTM(int(hidden_size*4), return_sequences=True)(lstm_out)\n",
    "    lstm_out = Dropout(dropout+0.2)(lstm_out)\n",
    "\n",
    "    # Attention mechanism\n",
    "    attn_weights = Dense(1, activation='tanh')(lstm_out)\n",
    "    attn_weights = Permute((2, 1))(attn_weights)\n",
    "    attn_weights = Activation('softmax')(attn_weights)\n",
    "    attn_weights = Permute((2, 1))(attn_weights)\n",
    "    attn_output = Multiply()([attn_weights, lstm_out])\n",
    "    context_vector = Lambda(lambda x: keras.backend.sum(x, axis=1))(attn_output)\n",
    "    \n",
    "    # Fully connected layers\n",
    "    fc1 = Dense(int(hidden_size), activation='relu')(context_vector)\n",
    "    fc1 = Dropout(dropout)(fc1)\n",
    "    fc2 = Dense(int(output_shape[1] * 2), activation='relu')(fc1)\n",
    "    fc2 = Dropout(dropout)(fc2)\n",
    "    predictions = Dense(output_shape[1], activation='linear')(fc2)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_Model(input_size, output_shape, hidden_size=64, dropout=0.2):\n",
    "    inputs = Input(shape=(input_size[1], input_size[2])) # (batch_size, time_step, features)\n",
    "\n",
    "    # GRU layers\n",
    "    gru_out = GRU(hidden_size, activation='tanh',return_sequences=True)(inputs)\n",
    "    gru_out = Dropout(dropout)(gru_out)\n",
    "\n",
    "    gru_out = GRU(int(hidden_size/2), activation='tanh',return_sequences=True)(gru_out)\n",
    "    gru_out = Dropout(dropout+0.1)(gru_out)\n",
    "    \n",
    "    gru_out = Flatten()(gru_out)\n",
    "\n",
    "    fc1 = Dense(hidden_size, activation='relu')(gru_out)\n",
    "    fc1 = Dropout(dropout)(fc1)\n",
    "    fc2 = Dense(int(output_shape[1] * 2), activation='relu')(fc1)\n",
    "    fc2 = Dropout(dropout)(fc2)\n",
    "    predictions = Dense(output_shape[1], activation='linear')(fc2)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Model(input_size, output_shape, hidden_size=64, dropout=0.2,kernel_size=3, pool_size=2):\n",
    "    inputs = Input(shape=(input_size[1], input_size[2])) # (batch_size, time_step, features)\n",
    "\n",
    "    # CNN layers\n",
    "    x = Conv1D(filters=hidden_size, kernel_size=kernel_size, activation='relu')(inputs)\n",
    "    x = MaxPooling1D(pool_size=pool_size)(x)\n",
    "\n",
    "    x = Conv1D(filters=int(hidden_size/2), kernel_size=kernel_size, activation='relu')(x)\n",
    "    x = MaxPooling1D(pool_size=pool_size)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    fc1 = Dense(hidden_size, activation='relu')(x)\n",
    "    fc1 = Dropout(dropout)(fc1)\n",
    "    fc2 = Dense(int(output_shape[1] * 2), activation='relu')(fc1)\n",
    "    fc2 = Dropout(dropout)(fc2)\n",
    "    predictions = Dense(output_shape[1], activation='linear')(fc2)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_Model(input_size, output_shape, hidden_size=64, dropout=0.2):\n",
    "    inputs = Input(shape=(input_size[1], input_size[2])) # (batch_size, time_step, features)\n",
    "\n",
    "    x = Dense(int(hidden_size*2), activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dropout(dropout)(x)\n",
    "\n",
    "    x = Dense(hidden_size, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dropout(dropout+0.1)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    fc1 = Dense(int(hidden_size), activation='relu')(x)\n",
    "    fc1 = Dropout(dropout)(fc1)\n",
    "\n",
    "    fc2 = Dense(int(output_shape[1] * 2), activation='relu')(fc1)\n",
    "    fc2 = Dropout(dropout)(fc2)\n",
    "\n",
    "    predictions = Dense(output_shape[1])(fc2)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "INPUT_SIZE = X_shape\n",
    "HIDDEN_SIZE = 180\n",
    "OUTPUT_SIZE = y_shape\n",
    "NUM_LAYERS = 6\n",
    "\n",
    "# Define model architecture\n",
    "KERNEL_SIZE = 3\n",
    "POOL_SIZE = 2\n",
    "DROPOUT = 0.1\n",
    "N_ESTIMATORS = 100\n",
    "\n",
    "# Define training parameters\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 30\n",
    "NUM_EPOCHS = 40\n",
    "GAMMA = 0.5\n",
    "STEP_SIZE = 3\n",
    "PATIENCE = 10\n",
    "MAX_TRIALS = 50\n",
    "SHUFFLE = True\n",
    "CHECKPOINT_DIR = \"./checkpoint_keras\"\n",
    "BACKUP_DIR = \"./backup_keras\"\n",
    "LOG_DIR = \"./logs_keras\"\n",
    "\n",
    "# DATA SPLIT\n",
    "TEST_SPLIT = 0.15\n",
    "VAL_SPLIT = 0.15\n",
    "TRAIN_SPLIT = 1 - TEST_SPLIT - VAL_SPLIT\n",
    "\n",
    "# Define random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = Y.shape[0]\n",
    "\n",
    "# Shuffle the data indices\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "num_validation_samples = int(num_samples * VAL_SPLIT)\n",
    "num_test_samples = int(num_samples * TEST_SPLIT)\n",
    "num_train_samples = num_samples - num_validation_samples - num_test_samples\n",
    "\n",
    "train_indices = indices[:num_train_samples]\n",
    "val_indices = indices[num_train_samples:num_train_samples + num_validation_samples]\n",
    "test_indices = indices[num_train_samples + num_validation_samples:]\n",
    "\n",
    "train_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': SHUFFLE,\n",
    "                'X_path': 'X_sequence.dat',\n",
    "                'Y_path': 'Y_sequence.dat',\n",
    "                'shapes': shapes,\n",
    "                'indices': train_indices}\n",
    "\n",
    "val_params = {'batch_size': BATCH_SIZE,\n",
    "              'shuffle': SHUFFLE,\n",
    "              'X_path': 'X_sequence.dat',\n",
    "              'Y_path': 'Y_sequence.dat',\n",
    "              'shapes': shapes,\n",
    "              'indices': val_indices}\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE,\n",
    "               'shuffle': SHUFFLE,\n",
    "               'X_path': 'X_sequence.dat',\n",
    "               'Y_path': 'Y_sequence.dat',\n",
    "               'shapes': shapes,\n",
    "               'indices': test_indices}\n",
    "\n",
    "train_dataset = DataGenerator(**train_params)\n",
    "val_dataset = DataGenerator(**val_params)\n",
    "test_dataset = DataGenerator(**test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = next(iter(train_dataset))\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create LSTM model\n",
    "def create_lstm_model(input_shape, output_size, hidden_size, dropout, optimizer, loss):\n",
    "    model = LSTM_Model(input_size=input_shape, output_shape=output_size, hidden_size=hidden_size, dropout=dropout)\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    model._name = 'LSTM_Model'\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# Function to create GRU model\n",
    "def create_gru_model(input_shape, output_size, hidden_size, dropout, optimizer, loss):\n",
    "    model = GRU_Model(input_size=input_shape, output_shape=output_size, hidden_size=hidden_size, dropout=dropout)\n",
    "    model._name = 'GRU_Model'\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# Function to create CNN model\n",
    "def create_cnn_model(input_shape, output_size, hidden_size, dropout, kernel_size, pool_size, optimizer, loss):\n",
    "    model = CNN_Model(input_size=input_shape, output_shape=output_size, hidden_size=hidden_size, dropout=dropout, kernel_size=kernel_size, pool_size=pool_size)\n",
    "    model._name = 'CNN_Model'\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# Function to create Random Forest model\n",
    "def create_nn_model(input_shape, output_size, hidden_size, dropout, optimizer, loss):\n",
    "    model = NN_Model(input_size=input_shape, output_shape=output_size, hidden_size=hidden_size, dropout=dropout)\n",
    "    model._name = 'NN_Model'\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# Function to create ensemble model\n",
    "def create_ensemble_model(input_shape, output_size, hidden_size, dropout, kernel_size, pool_size, optimizer, loss):\n",
    "    lstm_model = create_lstm_model(input_shape, output_size, hidden_size, dropout, optimizer, loss)\n",
    "    gru_model = create_gru_model(input_shape, output_size, hidden_size, dropout, optimizer, loss)\n",
    "    cnn_model = create_cnn_model(input_shape, output_size, hidden_size, dropout, kernel_size, pool_size, optimizer, loss)\n",
    "    nn_model = create_nn_model(input_shape, output_size, hidden_size, dropout, optimizer, loss)\n",
    "    models = [lstm_model, gru_model, cnn_model, nn_model]\n",
    "    input = Input(shape=(input_shape[1], input_shape[2]))\n",
    "    outputs = [model(input) for model in models]\n",
    "    ensemble_output = Average()(outputs)\n",
    "    ensemble_output = Dense(output_size[1], activation='linear')(ensemble_output)\n",
    "    ensemble_model = Model(inputs=input, outputs=ensemble_output)\n",
    "    ensemble_model._name = 'Ensemble_Model'\n",
    "    return ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses, optimizers\n",
    "\n",
    "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "LOSS = losses.MeanSquaredError()\n",
    "\n",
    "ensemble_model = create_ensemble_model(input_shape=INPUT_SIZE, output_size=OUTPUT_SIZE, hidden_size=HIDDEN_SIZE, dropout=DROPOUT, kernel_size=KERNEL_SIZE, pool_size=POOL_SIZE, optimizer=OPTIMIZER, loss=LOSS)\n",
    "ensemble_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[keras.metrics.RootMeanSquaredError(), keras.metrics.MeanAbsoluteError(), keras.metrics.MeanAbsolutePercentageError(), keras.metrics.MeanSquaredLogarithmicError(), keras.metrics.CosineSimilarity(axis=1, name='cosine_similarity')])\n",
    "print(ensemble_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(factor=GAMMA, patience=STEP_SIZE)\n",
    "\n",
    "checkpoint_filepath = CHECKPOINT_DIR + '/checkpoint_RoboStockModel_{epoch:02d}-{val_accuracy:.2f}.keras'\n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', mode='max', save_best_only=True, overwrite=True)\n",
    "\n",
    "backup_callback = BackupAndRestore(backup_dir=BACKUP_DIR)\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "if os.path.exists(LOG_DIR):\n",
    "    shutil.rmtree(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir {LOG_DIR} --host localhost --port 8888\n",
    "history = ensemble_model.fit(train_dataset, epochs=NUM_EPOCHS, steps_per_epoch=len(train_dataset), validation_data=val_dataset, validation_steps=len(val_dataset), callbacks=[backup_callback,early_stopping, reduce_lr, checkpoint_callback, tensorboard_callback], shuffle=SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = next(iter(test_dataset))\n",
    "\n",
    "output = ensemble_model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, output)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "test_loss = ensemble_model.evaluate(test_dataset, steps=len(test_dataset))\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.save('RoboStockModel.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.save_weights('./checkpoint/RoboStockModel_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_len = y_test.shape[0]\n",
    "\n",
    "fig, axs = plt.subplots(int(y_len/2),y_len-int(y_len/2),figsize=(50, 50))\n",
    "for i in range(int(y_len/2)):\n",
    "    for j in range(y_len-int(y_len/2)):\n",
    "        axs[i][j].scatter(y_test[i+j], output[i+j])\n",
    "fig.suptitle('y_true vs y_pred')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_len = y_test.shape[0]\n",
    "\n",
    "fig, axs = plt.subplots(int(y_len/2),y_len-int(y_len/2),figsize=(50, 50))\n",
    "X_axis = np.arange(0, len(y_test[0]), 1)\n",
    "for i in range(int(y_len/2)):\n",
    "    for j in range(y_len-int(y_len/2)):\n",
    "        axs[i][j].plot(X_axis,y_test[i+j], color = 'red', label = 'y_test')\n",
    "        axs[i][j].plot(X_axis,output[i+j], color = 'blue', label = 'y_pred', linestyle = 'dashed')\n",
    "fig.suptitle('y_true vs y_pred')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
